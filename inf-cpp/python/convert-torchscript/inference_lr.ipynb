{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TRKXINPUTDIR'] = '/fs/ess/PLS0151/trackPred/' # better change to your copy of the dataset.\n",
    "os.environ['TRKXOUTPUTDIR'] = '../run200' # change to your own directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.0\n"
     ]
    }
   ],
   "source": [
    "import torch; \n",
    "print(torch.version.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True 11.0\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'random_edge_slice_v2' from 'trained_models_v0.GNN.model.utils' (/users/PLS0129/ysu0053/torchScript/trained_models_v0/GNN/model/utils.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-128d462bbdad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;31m#from exatrkx import VanillaFilter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtrained_models_v0\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFilter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvanilla_filter\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVanillaFilter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtrained_models_v0\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGNN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magnn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mResAGNN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;31m#from exatrkx import ResAGNN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/torchScript/trained_models_v0/GNN/model/Models/agnn.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheckpoint\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcheckpoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgnn_base\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGNNBase\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmake_mlp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/torchScript/trained_models_v0/GNN/model/gnn_base.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_edge_slice_v2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'random_edge_slice_v2' from 'trained_models_v0.GNN.model.utils' (/users/PLS0129/ysu0053/torchScript/trained_models_v0/GNN/model/utils.py)"
     ]
    }
   ],
   "source": [
    "# system import\n",
    "import pkg_resources\n",
    "import yaml\n",
    "import pprint\n",
    "import random\n",
    "random.seed(1234)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "#import psutil \n",
    "# %matplotlib widget\n",
    "\n",
    "# 3rd party\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data\n",
    "from trackml.dataset import load_event\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "import torch.utils.data as data_utils\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from trackml.dataset import load_event\n",
    "from exatrkx.src.processing.utils.detector_utils import load_detector\n",
    "from exatrkx.src.processing.utils.cell_utils import get_one_event\n",
    "\n",
    "# for embedding\n",
    "from exatrkx import LayerlessEmbedding\n",
    "#from trained_models_v0.Embedding.model.Models.layerless_embedding import LayerlessEmbedding\n",
    "from exatrkx.src import utils_torch\n",
    "#import pynndescent\n",
    "# for filtering\n",
    "#from exatrkx import VanillaFilter\n",
    "from trained_models_v0.Filter.model.Models.vanilla_filter import VanillaFilter\n",
    "from trained_models_v0.GNN.model.Models.agnn import ResAGNN\n",
    "#from exatrkx import ResAGNN\n",
    "\n",
    "# for GNN\n",
    "import tensorflow as tf\n",
    "from graph_nets import utils_tf\n",
    "from exatrkx import SegmentClassifier\n",
    "import sonnet as snt\n",
    "\n",
    "# for labeling\n",
    "from exatrkx.scripts.tracks_from_gnn import prepare as prepare_labeling\n",
    "from exatrkx.scripts.tracks_from_gnn import clustering as dbscan_clustering\n",
    "#from exatrkx.scripts.tracks_from_gnn import hclustering as hdbscan_clustering\n",
    "import networkx as nx\n",
    "import cudf\n",
    "import cugraph\n",
    "# track efficiency\n",
    "from trackml.score import _analyze_tracks\n",
    "from exatrkx.scripts.eval_reco_trkx import make_cmp_plot, pt_configs, eta_configs\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import exatrkx\n",
    "print(exatrkx.__file__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup some hyperparameters and event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_ckpt_dir = '/users/PLS0129/ysu0053/torchScript/trained_models_v0/Embedding/trained/checkpoints/last.ckpt'\n",
    "filter_ckpt_dir = '/users/PLS0129/ysu0053/torchScript/trained_models_v0/Filter/trained/checkpoints/last.ckpt'\n",
    "gnn_ckpt_dir = '/users/PLS0129/ysu0053/torchScript/trained_models_v0/GNN/trained/checkpoints/last.ckpt'\n",
    "plots_dir = '../run200' # needs to change...\n",
    "ckpt_idx = -1 # which GNN checkpoint to load\n",
    "dbscan_epsilon, dbscan_minsamples = 0.25, 2 # hyperparameters for DBScan\n",
    "min_hits = 5 # minimum number of hits associated with a particle to define \"reconstructable particles\"\n",
    "frac_reco_matched, frac_truth_matched = 0.5, 0.5 # parameters for track matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detector_path='/fs/ess/PLS0151/trackPred/detectors.csv'\n",
    "input_path = \"/fs/ess/PLS0151/trackPred/train_100_events/\"\n",
    "detector_orig, detector_proc = load_detector(detector_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_one_event(evtid):\n",
    "    event_file = os.path.join(input_path, 'event{:09}'.format(evtid))\n",
    "    hits, particles, truth = load_event(event_file, parts=['hits', 'particles', 'truth'])\n",
    "\n",
    "    r = np.sqrt(hits.x**2 + hits.y**2)\n",
    "    phi = np.arctan2(hits.y, hits.x)\n",
    "    hits = hits.assign(r=r, phi=phi)\n",
    "    hits = hits.merge(truth, on='hit_id')\n",
    "    hits = hits[hits['particle_id'] != 0]\n",
    "    hits = hits.merge(particles, on='particle_id', how='left')\n",
    "    hits = hits[hits.nhits >= min_hits]\n",
    "    particles = particles[particles.nhits >= min_hits]\n",
    "\n",
    "    angles = get_one_event(event_file, detector_orig, detector_proc)\n",
    "    hits = hits.merge(angles, on='hit_id')\n",
    "\n",
    "    cell_features = ['cell_count', 'cell_val', 'leta', 'lphi', 'lx', 'ly', 'lz', 'geta', 'gphi']\n",
    "    feature_scale = np.array([1000, np.pi, 1000])\n",
    "    hid = hits['hit_id'].to_numpy()\n",
    "    x = hits[['r', 'phi', 'z']].to_numpy() / feature_scale\n",
    "    cell_data = hits[cell_features].to_numpy()\n",
    "    return hid, x, cell_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_hits = 5\n",
    "evtid =1000\n",
    "hid, x, cell_data = prepare_one_event(evtid)\n",
    "data = Data(hid=torch.from_numpy(hid),\n",
    "            x=torch.from_numpy(x).float(),\n",
    "            cell_data=torch.from_numpy(cell_data).float(),\n",
    "            pin_memory=True).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_ckpt = torch.load(embed_ckpt_dir, map_location='cpu')\n",
    "e_config = e_ckpt['hyper_parameters']\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "pp.pprint(e_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_config = e_ckpt['hyper_parameters']\n",
    "e_config['clustering'] = 'build_edges'\n",
    "e_config['knn_val'] = 500\n",
    "e_config['r_val'] = 1.6\n",
    "e_config['in_channels'] = 3\n",
    "pp.pprint(e_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the checkpoint and put the model in the evaluation state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_model = LayerlessEmbedding(e_config).to(device)\n",
    "e_model.load_state_dict(e_ckpt[\"state_dict\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_model.eval()\n",
    "#e_model.half()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.jit.optimized_execution(True):\n",
    "    e_script = e_model.to_torchscript()\n",
    "#fe_script = torch.jit.freeze(e_script)\n",
    "# Save to file\n",
    "torch.jit.save(e_script, 'embed.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_script.code\n",
    "#fe_script.hparams['r_val']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_script = torch.jit.load('embed.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Map each hit to the embedding space, return the embeded parameters for each hit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cat([data.cell_data, data.x], axis=-1).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_input = torch.full((5, 3), 1.0).to(device)\n",
    "e_input = data.x\n",
    "e_df = pd.read_csv(\"data/in_e.csv\", header=None, dtype=np.float32)\n",
    "e= e_df.values\n",
    "e_input = torch.tensor(e, dtype=torch.float32).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "with torch.no_grad():\n",
    "    #with torch.cuda.amp.autocast():\n",
    "    #spatial = e_model(e_input)\n",
    "    spatial = e_script(e_input)\n",
    "spatial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From embeddeding space form doublets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`r_val = 1.7` and `knn_val = 500` are the hyperparameters to be studied.\n",
    "\n",
    "* `r_val` defines the radius of the clustering method\n",
    "* `knn_val` defines the number of maximum neighbors in the embedding space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#e_spatial = utils_torch.build_edges_baseline(spatial.to(device), e_model.hparams['r_val'], e_model.hparams['knn_val'])\n",
    "e_spatial = utils_torch.build_edges(spatial.to(device),1.6 ,500) #e_model.hparams['r_val']\n",
    "e_spatial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing edges that point from outer region to inner region, which almost removes half of edges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "R_dist = torch.sqrt(data.x[:,0]**2 + data.x[:,2]**2) # distance away from origin...\n",
    "e_spatial = e_spatial[:, (R_dist[e_spatial[0]] <= R_dist[e_spatial[1]])]\n",
    "e_spatial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_ckpt = torch.load(filter_ckpt_dir, map_location='cpu')\n",
    "f_config = f_ckpt['hyper_parameters']\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "#pp.pprint(f_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_config['train_split'] = [0, 0, 1]\n",
    "f_config['filter_cut'] = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_model = VanillaFilter(f_config).to(device)\n",
    "# f_model = f_model.load_from_checkpoint(filter_ckpt_dir, hparams=f_config)\n",
    "f_model.load_state_dict(f_ckpt['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_model.eval()\n",
    "#f_model.half()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.jit.optimized_execution(True):\n",
    "    f_script = f_model.to_torchscript()\n",
    "#ff_script = torch.jit.freeze(f_script)\n",
    "# Save to file\n",
    "torch.jit.save(f_script, 'filter.pt')\n",
    "#f_script.graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%time\n",
    "#emb = None # embedding information was not used in the filtering stage.\n",
    "#output = f_model(torch.cat([data.cell_data, data.x], axis=-1), e_spatial, emb).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "emb = None # embedding information was not used in the filtering stage.\n",
    "chunks = 8\n",
    "output_list = []\n",
    "for j in range(chunks):\n",
    "    subset_ind = torch.chunk(torch.arange(e_spatial.shape[1]), chunks)[j]\n",
    "    with torch.no_grad():\n",
    "        with torch.cuda.amp.autocast():\n",
    "            #output = f_script(torch.cat([data.cell_data, data.x], axis=-1).to(device), e_spatial[:, subset_ind]).squeeze()\n",
    "            output = f_script(e_input.to(device), e_spatial[:, subset_ind]).squeeze()\n",
    "            #output = f_model(torch.cat([data.cell_data, data.x], axis=-1).to(device), e_spatial[:, subset_ind]).squeeze() \n",
    "    output_list.append(output)\n",
    "output = torch.cat(output_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = torch.sigmoid(output)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.shape, e_spatial.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this plot may need some time to load...\n",
    "plt.hist(output.detach().to('cpu').numpy(), bins=100, histtype='step', lw=2)\n",
    "plt.show()\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The filtering network assigns a score to each edge. In the end, edges with socres > `filter_cut` are selected to construct graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_list = e_spatial[:, output.to('cpu') > f_model.hparams['filter_cut']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_list.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Form a graph\n",
    "Now moving TensorFlow for GNN inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_nodes = data.x.shape[0]\n",
    "n_edges = edge_list.shape[1]\n",
    "#nodes = data.x.numpy().astype(np.float32)\n",
    "nodes = data.x #.cpu().numpy().astype(np.float32)\n",
    "#edge_list = edge_list.cpu().numpy().astype(np.int32)\n",
    "#edges = np.zeros((n_edges, 1), dtype=np.float32)\n",
    "#senders = edge_list[0].cpu()\n",
    "#receivers = edge_list[1].cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input_datadict = {\n",
    "#    \"n_node\": n_nodes,\n",
    "#    \"n_edge\": n_edges,\n",
    "#    \"nodes\": nodes,\n",
    "#    \"edges\": edges,\n",
    "#    \"senders\": senders,\n",
    "#    \"receivers\": receivers,\n",
    "#    \"globals\": np.array([n_nodes], dtype=np.float32)\n",
    "#}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input_graph = utils_tf.data_dicts_to_graphs_tuple([input_datadict])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply GNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#num_processing_steps_tr = 8\n",
    "#optimizer = snt.optimizers.Adam(0.001)\n",
    "#model = SegmentClassifier()\n",
    "\n",
    "#output_dir = gnn_ckpt_dir\n",
    "#checkpoint = tf.train.Checkpoint(optimizer=optimizer, model=model)\n",
    "#ckpt_manager = tf.train.CheckpointManager(checkpoint, directory=output_dir, max_to_keep=10)\n",
    "#status = checkpoint.restore(ckpt_manager.checkpoints[ckpt_idx]).expect_partial()\n",
    "#print(\"Loaded {} checkpoint from {}\".format(ckpt_idx, output_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(\"last.ckpt\", map_location=device)\n",
    "hparams = checkpoint[\"hyper_parameters\"]\n",
    "state_dict = checkpoint[\"state_dict\"]\n",
    "\n",
    "g_model = ResAGNN(hparams).to(device)\n",
    "g_model.load_state_dict(state_dict)\n",
    "g_model.eval()\n",
    "#g_model.half()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = (data.x, edge_list)\n",
    "g_script = g_model.to_torchscript(file_path=\"gnn_script.pt\",example_inputs=input_data)\n",
    "#g_script = torch.jit.script(model, input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.jit.optimized_execution(True):\n",
    "    g_script = g_model.to_torchscript()\n",
    "#g_script = torch.jit.freeze(g_script)\n",
    "# Save to file\n",
    "torch.jit.save(g_script, 'gnn.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%time\n",
    "#outputs_gnn = model(nodes, edge_list)\n",
    "#output_graph = outputs_gnn[-1]\n",
    "\n",
    "#with torch.no_grad():\n",
    "#   with torch.cuda.amp.autocast():\n",
    "#       gnnoutput = g_model(data.x,edge_list)\n",
    "#nnoutput = torch.sigmoid(gnnoutput).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "with torch.no_grad():\n",
    "    with torch.cuda.amp.autocast():\n",
    "        gnnoutput = g_script(data.x, edge_list)\n",
    "        #gnnoutput = g_model(data.x, edge_list)\n",
    "gnnoutput = torch.sigmoid(gnnoutput).cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Track labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input_matrix = prepare_labeling(tf.squeeze(output_graph.edges).cpu().numpy(), senders, receivers, n_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload \n",
    "import exatrkx.scripts.tracks_from_gnn\n",
    "reload(exatrkx.scripts.tracks_from_gnn)\n",
    "from exatrkx.scripts.tracks_from_gnn import clustering as dbscan_clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#predict_tracks = dbscan_clustering(data.hid.cpu(), input_matrix, dbscan_epsilon, dbscan_minsamples)\n",
    "hids = data.hid.cpu().numpy()\n",
    "cut_edges = hids[edge_list.cpu().numpy()][:,gnnoutput > 0.75]\n",
    "cut_df = cudf.DataFrame(cut_edges.T)\n",
    "G=cugraph.Graph()\n",
    "G.from_cudf_edgelist(cut_df,source=0, destination=1, edge_attr=None)\n",
    "labels = cugraph.components.connectivity.weakly_connected_components(G)\n",
    "predict_tracks_df = labels.to_pandas()\n",
    "predict_tracks_df.columns = [\"track_id\",\"hit_id\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Track Efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_file = os.path.join(input_path, 'event{:09}'.format(evtid))\n",
    "hits, particles, truth = load_event(event_file, parts=['hits', 'particles', 'truth'])\n",
    "hits = hits.merge(truth, on='hit_id', how='left')\n",
    "hits = hits[hits.particle_id > 0] # remove noise hits\n",
    "hits = hits.merge(particles, on='particle_id', how='left')\n",
    "hits = hits[hits.nhits >= min_hits]\n",
    "particles = particles[particles.nhits >= min_hits]\n",
    "par_pt = np.sqrt(particles.px**2 + particles.py**2)\n",
    "momentum = np.sqrt(particles.px**2 + particles.py**2 + particles.pz**2)\n",
    "ptheta = np.arccos(particles.pz/momentum)\n",
    "peta = -np.log(np.tan(0.5*ptheta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracks = _analyze_tracks(hits, predict_tracks_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "purity_rec = np.true_divide(tracks['major_nhits'], tracks['nhits'])\n",
    "purity_maj = np.true_divide(tracks['major_nhits'], tracks['major_particle_nhits'])\n",
    "good_track = (frac_reco_matched < purity_rec) & (frac_truth_matched < purity_maj)\n",
    "\n",
    "matched_pids = tracks[good_track].major_particle_id.values\n",
    "score = tracks['major_weight'][good_track].sum()\n",
    "\n",
    "n_recotable_trkx = particles.shape[0]\n",
    "n_reco_trkx = tracks.shape[0]\n",
    "n_good_recos = np.sum(good_track)\n",
    "matched_idx = particles.particle_id.isin(matched_pids).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Processed {} events from {}\".format(evtid, input_path))\n",
    "print(\"Reconstructable tracks:         {}\".format(n_recotable_trkx))\n",
    "print(\"Reconstructed tracks:           {}\".format(n_reco_trkx))\n",
    "print(\"Reconstructable tracks Matched: {}\".format(n_good_recos))\n",
    "print(\"Tracking efficiency:            {:.4f}\".format(n_good_recos/n_recotable_trkx))\n",
    "print(\"Tracking purity:               {:.4f}\".format(n_good_recos/n_reco_trkx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_cmp_plot_fn = partial(make_cmp_plot, xlegend=\"Matched\", ylegend=\"Reconstructable\",\n",
    "                    ylabel=\"Events\", ratio_label='Track efficiency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_cmp_plot_fn(par_pt[matched_idx], par_pt,\n",
    "                 configs=pt_configs,\n",
    "                 xlabel=\"pT [GeV]\",\n",
    "                 outname=os.path.join(plots_dir, \"{}_pt\".format(evtid)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_cmp_plot_fn(peta[matched_idx], peta,\n",
    "                 configs=eta_configs,\n",
    "                 xlabel=r\"$\\eta$\",\n",
    "                 outname=os.path.join(plots_dir, \"{}_eta\".format(evtid)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_one_event(evtid):\n",
    "    event_file = os.path.join(input_path, 'event{:09}'.format(evtid))\n",
    "    hits, particles, truth = load_event(event_file, parts=['hits', 'particles', 'truth'])\n",
    "\n",
    "    r = np.sqrt(hits.x**2 + hits.y**2)\n",
    "    phi = np.arctan2(hits.y, hits.x)\n",
    "    hits = hits.assign(r=r, phi=phi)\n",
    "    hits = hits.merge(truth, on='hit_id')\n",
    "    hits = hits[hits['particle_id'] != 0]\n",
    "    hits = hits.merge(particles, on='particle_id', how='left')\n",
    "    hits = hits[hits.nhits >= min_hits]\n",
    "    particles = particles[particles.nhits >= min_hits]\n",
    "\n",
    "    angles = get_one_event(event_file, detector_orig, detector_proc)\n",
    "    hits = hits.merge(angles, on='hit_id')\n",
    "\n",
    "    cell_features = ['cell_count', 'cell_val', 'leta', 'lphi', 'lx', 'ly', 'lz', 'geta', 'gphi']\n",
    "    feature_scale = np.array([1000, np.pi, 1000])\n",
    "    hid = hits['hit_id'].to_numpy()\n",
    "    x = hits[['r', 'phi', 'z']].to_numpy() / feature_scale\n",
    "    cell_data = hits[cell_features].to_numpy()\n",
    "    return hid, x, cell_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evtid =1000\n",
    "hid, x, cell_data = prepare_one_event(evtid)\n",
    "data = Data(hid=torch.from_numpy(hid),\n",
    "            x=torch.from_numpy(x).float(),\n",
    "            cell_data=torch.from_numpy(cell_data).float(),\n",
    "            pin_memory=True).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_script.code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_script._parameters.values()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ExaTrkX",
   "language": "python",
   "name": "trkx"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
